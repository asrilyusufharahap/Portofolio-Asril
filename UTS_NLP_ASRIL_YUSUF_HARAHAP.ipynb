{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNaXz+XegAxPe7g2UdZYqXu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asrilyusufharahap/Portofolio-Asril/blob/main/UTS_NLP_ASRIL_YUSUF_HARAHAP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJE0FHrBVveO",
        "outputId": "41896bad-fb5d-44f2-f16b-ba70a093a318"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOMOR 1: TEXT PREPROCESSING ====\n",
            "\n",
            "1.a Case Folding:\n",
            "pendidikan adalah kunci utama menuju kesuksesan, meskipun tantangan selalu datang.\n",
            "\n",
            "1.b Tokenization:\n",
            "['pendidikan', 'adalah', 'kunci', 'utama', 'menuju', 'kesuksesan', ',', 'meskipun', 'tantangan', 'selalu', 'datang', '.']\n",
            "\n",
            "1.c Setelah Stopword Removal:\n",
            "['pendidikan', 'kunci', 'utama', 'kesuksesan', 'tantangan']\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "==== NOMOR 2: TOKENIZATION DENGAN spaCy ====\n",
            "\n",
            "2. Hasil Tokenization spaCy:\n",
            "Pendidikan\n",
            "adalah\n",
            "kunci\n",
            "utama\n",
            "menuju\n",
            "kesuksesan\n",
            ",\n",
            "meskipun\n",
            "tantangan\n",
            "selalu\n",
            "datang\n",
            ".\n",
            "\n",
            "\n",
            "\n",
            "==== NOMOR 3: POS TAGGING ====\n",
            "\n",
            "3. Hasil POS Tagging (kata, POS, tag):\n",
            "Natural      ADJ      JJ      \n",
            "language     NOUN     NN      \n",
            "processing   NOUN     NN      \n",
            "helps        VERB     VBZ     \n",
            "computers    NOUN     NNS     \n",
            "understand   VERB     VB      \n",
            "human        ADJ      JJ      \n",
            "language     NOUN     NN      \n",
            "clearly      ADV      RB      \n",
            ".            PUNCT    .       \n",
            "\n",
            "\n",
            "\n",
            "==== NOMOR 4: NAMED ENTITY RECOGNITION (NER) ====\n",
            "\n",
            "4. Teks berita:\n",
            "Indonesia's central bank kept interest rates steady on Tuesday as inflation stayed within the target range, Governor Budi said in Jakarta.\n",
            "\n",
            "4. Hasil NER (Entitas dan Jenisnya):\n",
            "Entitas: Indonesia             Label: GPE\n",
            "Entitas: Tuesday               Label: DATE\n",
            "Entitas: Budi                  Label: PERSON\n",
            "Entitas: Jakarta               Label: GPE\n",
            "\n",
            "\n",
            "\n",
            "==== NOMOR 5: STEMMING vs LEMMATIZATION ====\n",
            "\n",
            "Kata         Stem (Porter)   Lemma       \n",
            "studies      studi           study       \n",
            "studying     studi           studying    \n",
            "better       better          better      \n",
            "runs         run             run         \n",
            "children     children        child       \n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import SnowballStemmer, PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Download resource NLTK (bisa di-comment setelah berhasil)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab') # Added this line to download the missing resource\n",
        "\n",
        "# Untuk spaCy, pastikan model ini sudah di-download di terminal:\n",
        "# python -m spacy download en_core_web_sm\n",
        "\n",
        "\n",
        "# ======================\n",
        "# 1. TEXT PREPROCESSING\n",
        "# ======================\n",
        "\n",
        "print(\"==== NOMOR 1: TEXT PREPROCESSING ====\\n\")\n",
        "\n",
        "text = \"Pendidikan adalah kunci utama menuju kesuksesan, meskipun tantangan selalu datang.\"\n",
        "\n",
        "# a. Case folding\n",
        "text_lower = text.lower()\n",
        "print(\"1.a Case Folding:\")\n",
        "print(text_lower)\n",
        "print()\n",
        "\n",
        "# b. Tokenization\n",
        "tokens = word_tokenize(text_lower)\n",
        "print(\"1.b Tokenization:\")\n",
        "print(tokens)\n",
        "print()\n",
        "\n",
        "# c. Stopword removal (bahasa Indonesia)\n",
        "stop_words = set(stopwords.words('indonesian'))\n",
        "tokens_no_sw = [t for t in tokens if t.isalpha() and t not in stop_words]\n",
        "print(\"1.c Setelah Stopword Removal:\")\n",
        "print(tokens_no_sw)\n",
        "print()\n",
        "\n",
        "# d. Stemming dengan SnowballStemmer bahasa Indonesia\n",
        "# SnowballStemmer does not support 'indonesian'. For Indonesian stemming, consider libraries like Sastrawi.\n",
        "# stemmer_id = SnowballStemmer(\"indonesian\")\n",
        "# stems = [stemmer_id.stem(t) for t in tokens_no_sw]\n",
        "# print(\"1.d Hasil Stemming:\")\n",
        "# print(stems)\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# 2. TOKENIZATION MENGGUNAKAN PUSTAKA spaCy\n",
        "# ============================================\n",
        "\n",
        "print(\"==== NOMOR 2: TOKENIZATION DENGAN spaCy ====\\n\")\n",
        "\n",
        "# Pakai model kosong multibahasa, cukup untuk tokenisasi\n",
        "nlp_tokenizer = spacy.blank(\"xx\")\n",
        "\n",
        "text2 = \"Pendidikan adalah kunci utama menuju kesuksesan, meskipun tantangan selalu datang.\"\n",
        "doc2 = nlp_tokenizer(text2)\n",
        "\n",
        "tokens_spacy = [token.text for token in doc2]\n",
        "\n",
        "print(\"2. Hasil Tokenization spaCy:\")\n",
        "for t in tokens_spacy:\n",
        "    print(t)\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# 3. POS TAGGING DENGAN spaCy + OUTPUT TAG\n",
        "# ============================================\n",
        "\n",
        "print(\"==== NOMOR 3: POS TAGGING ====\\n\")\n",
        "\n",
        "# Load model bahasa Inggris untuk POS Tagging\n",
        "nlp_pos = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text3 = \"Natural language processing helps computers understand human language clearly.\"\n",
        "doc3 = nlp_pos(text3)\n",
        "\n",
        "print(\"3. Hasil POS Tagging (kata, POS, tag):\")\n",
        "for token in doc3:\n",
        "    print(f\"{token.text:12} {token.pos_:8} {token.tag_:8}\")\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 4. NAMED ENTITY RECOGNITION (NER) DARI TEKS BERITA PENDEK\n",
        "# =========================================================\n",
        "\n",
        "print(\"==== NOMOR 4: NAMED ENTITY RECOGNITION (NER) ====\\n\")\n",
        "\n",
        "# Pakai model yang sama (en_core_web_sm)\n",
        "nlp_ner = nlp_pos\n",
        "\n",
        "news_text = (\n",
        "    \"Indonesia's central bank kept interest rates steady on Tuesday \"\n",
        "    \"as inflation stayed within the target range, Governor Budi said in Jakarta.\"\n",
        ")\n",
        "\n",
        "doc4 = nlp_ner(news_text)\n",
        "\n",
        "print(\"4. Teks berita:\")\n",
        "print(news_text)\n",
        "print(\"\\n4. Hasil NER (Entitas dan Jenisnya):\")\n",
        "for ent in doc4.ents:\n",
        "    print(f\"Entitas: {ent.text:20}  Label: {ent.label_}\")\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "\n",
        "# ====================================================\n",
        "# 5. PERBANDINGAN STEMMING DAN LEMMATIZATION (PYTHON)\n",
        "# ====================================================\n",
        "\n",
        "print(\"==== NOMOR 5: STEMMING vs LEMMATIZATION ====\\n\")\n",
        "\n",
        "words = [\"studies\", \"studying\", \"better\", \"runs\", \"children\"]\n",
        "\n",
        "porter = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "print(f\"{'Kata':12} {'Stem (Porter)':15} {'Lemma':12}\")\n",
        "for w in words:\n",
        "    stem = porter.stem(w)\n",
        "    lemma = lemmatizer.lemmatize(w)  # default: noun\n",
        "    print(f\"{w:12} {stem:15} {lemma:12}\")\n"
      ]
    }
  ]
}